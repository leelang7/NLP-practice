## 1. 모델 학습을 위한 데이터 분할

이번 과정에서는 감정 분석 모델을 만들고 활용하는 방법을 배워보도록 하겠습니다. 본격적으로 모델을 만들어 보기에 앞서 주어진 데이터를 **학습 데이터**와 **평가 데이터**로 나누는 방법에 대해서 배워보도록 하겠습니다.

**학습 데이터**란 감정 분석 모델을 훈련 시키기 위해 문장과 해당 문장의 감정이 포함되어 있는 데이터셋을 의미합니다.

**평가 데이터**란 학습된 모델의 성능을 평가하기 위해 학습에 포함되지 않은 데이터셋을 의미합니다.

이번 과정에서 `Emotions dataset for NLP` 데이터셋을 활용하여 **문장별 감정 분석**을 진행해 볼 예정입니다. 본 데이터셋의 각 줄은 아래와 같이 `문장;감정`의 형태로 구성이 되어 있습니다.

```
i didnt feel humiliated;sadness
```

## 지시사항

1. `emotions_train.txt`에 들어 있는 데이터를 불러와서 각 줄을 `(문장, 감정)` 형태의 튜플로 `data` 리스트에 저장하세요.
   - 각 줄에서 문장과 감정은 `;`으로 구분되어 있다는 점에 유의하세요.
   - 모든 줄의 끝에 존재하는 `\n`은 제거하세요.
2. `scikit-learn`의 `train_test_split` 함수를 사용하여, 학습 데이터와 평가 데이터를 8:2의 기준으로 분할하세요.
   - 매개변수에서 `test_size`는 0.2, `random_state`는 7로 설정하세요.
3. 학습 데이터의 문장을 `Xtrain` 변수에, 감정을 `Ytrain` 변수에 저장하세요.
4. 학습 데이터 내 **문장의 개수**와 **감정의 종류**를 출력하세요.
5. 평가 데이터의 문장을 `Xtest` 변수에 감정을 `Ytest` 변수에 저장하세요.
6. 평가 데이터 내 **문장의 개수**를 출력하세요.



## 2. 나이브 베이즈 학습

나이브 베이즈 기법에서는 각 감정 내 단어의 **가능도(likelihood)** 를 기반으로 문장의 감정을 예측합니다. 감정 내 단어의 가능도는 아래와 같은 공식으로 계산을 할 수 있습니다.

P^(단어|감정)=감정 내 단어의 빈도수감정 내 모든 단어의 빈도수*P*^(**단어****|****감정**)=**감정** **내** **모든** **단어의** **빈도수****감정** **내** **단어의** **빈도수**

이번 실습에서는 먼저 단어들의 가능도를 구하는 함수를 작성하도록 하겠습니다. 학습 데이터로 `Emotions dataset for NLP`데이터셋을 사용합니다.

## 지시사항

1. `cal_partial_freq()` 함수를 완성하세요. 함수는 텍스트 데이터(`texts`)와 특정 감정(`emotion`)을 매개변수로 가지며, 해당 감정을 나타내는 문서를 `filtered_texts`에 저장합니다. 이를 사용해서, 입력되는 감정을 표현하는 문서 내 각 단어의 빈도수를 `partial_freq` 딕셔너리 변수에 저장하세요.
2. `cal_total_freq` 함수를 완성하세요. 이 함수는 1번에서 생성된 `partial_freq` 딕셔너리를 입력받아, 특정 감정별 문서 내 전체 단어의 빈도수를 계산하여 반환합니다.
3. `Emotions dataset for NLP` 데이터셋에서 `joy`라는 감정을 표현하는 문장에서 단어 `happy`가 발생할 **가능도**를 `joy_likelihood` 변수에 저장하세요.
4. `Emotions dataset for NLP` 데이터셋에서 `sadness`라는 감정을 표현하는 문장에서 단어 `happy`가 발생할 **가능도**를 `sad_likelihood` 변수에 저장하세요.
5. `Emotions dataset for NLP` 데이터셋에서 `surprise`라는 감정을 표현하는 문장에서 단어 `can`이 발생할 **가능도**를 `sad_likelihood` 변수에 저장하세요.



## 3. 나이브 베이즈 기반 문장 감정 예측

이전 실습을 통해서 나이브 베이즈에서 단어의 가능도를 어떻게 계산하는지 확인하였습니다. 이번에는 나이브 베이즈의 나머지 부분들을 모두 구현하여, 주어진 **문장의 감정**을 예측하도록 하겠습니다.

## 지시사항

1. [실습 2]에서 구현한 `cal_partial_freq`와 `cal_total_freq` 함수를 완성하세요.
2. 입력되는 `data` 내 특정 감정의 **로그 발생 확률** 을 반환해 주는 `cal_prior_prob` 함수를 완성하세요.
   - 로그는 `np.log()` 함수를 사용하여 구할 수 있습니다.
3. 매개변수 `data`를 학습 데이터로 사용하여 `sent`의 각 **감정별 로그 확률**을 계산해 주는 `predict_emotion`함수를 완성하세요. 감정별 로그 확률 계산을 위해 **단어의 로그 가능도**를 사용하세요. 그리고 **스무딩 값을 10**으로 설정해 주세요. 결과는 `(감정, 확률)`의 형태로 `predictions` 리스트에 저장하세요. 이 중 **확률값이 가장 높은 `(감정, 확률)` 튜플**을 반환하세요.
4. `Emotions dataset for NLP 데이터셋`을 학습 데이터로 사용하여, 3번에서 작성한 함수로 `test_sent`의 감정을 예측한 결과를 확인하세요.



## 4. scikit-learn을 통한 나이브 베이즈 감정 분석

지금까지 직접 나이브 베이즈를 구현해봤습니다. 이제 나이브 베이즈의 작동 원리를 파악했으니, `scitkit-learn`을 통해 나이브 베이즈를 보다 간편하게 학습하고 예측하는 방법에 대해 배워보도록 하겠습니다.

`scikit-learn`이란 각종 머신 러닝 모델을 간편하게 사용할 수 있게 도와주는 파이썬 라이브러리입니다.

## 지시사항

1. scikit-learn을 통해 학습 데이터의 문장을 단어의 빈도수 벡터로 생성해 주는 `CountVectorizer` 객체인 변수 `cv`를 만들고, `fit_transform` 메소드로 `train_data`를 변환하세요. 변환 결과를 `transformed_text`에 저장하세요.
   - 학습 문장과 학습 감정은 각각 `train_data`와 `train_emotion`에 저장되어 있습니다.
2. `MultinomialNB` 객체인 변수 `clf`를 생성하고, `fit` 메소드로 2번에서 변환된 `train_data`와 `train_emotion`을 학습하세요.
3. `test_data` 안에 존재하는 5개 문장의 감정을 예측하고, 결과를 `test_result` 변수에 저장하세요. `cv.transform`을 이용해 단어의 빈도수 벡터를 만든 후, `clf.predict`를 이용해 감정을 예측하면 됩니다.



## 5. 나이브 베이즈 기반 감정 분석 서비스

이번 실습에서는 `flask`를 활용하여, 웹 기반으로 나이브 베이즈 모델을 호출하는 방법에 대해 배워보도록 하겠습니다.

`nb_flask.py`에는 `flask`로 작성된 웹 서버의 코드가 포함되어 있습니다. 웹 서버 시작 시, 이전 실습에서 학습된 `CountVectorizer` 객체인 `cv`와 `MultinomialNB` 객체인 `clf`가 로딩됩니다.

## 지시사항

1. `nb_flask.py` 파일 내 `predict` 함수를 완성하세요. 함수 내 `query` 변수는 웹 서버로 전달된 문자열 리스트를 의미합니다. 여기서 기존에 학습된 `CountVectorizer` 객체인 `cv`와 `MultinomialNB` 객체인 `clf`를 사용해서 `query`에 전달된 리스트 내 각 문장의 감정을 예측하는 코드를 작성하세요.
2. 1번에서 예측된 결과를 `response` 딕셔너리에 `문장의 순서: 예측된 감정` 형태로 저장하세요.
3. 실행 버튼을 눌러 `main.py` 내 `test_data`의 감정을 웹 서버를 통해 예측하세요.