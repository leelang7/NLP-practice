## 1. Bag of words 기반 문서 벡터 생성

이번 실습에서는 `scikit-learn`의 `CountVectorizer`를 사용하여, bag of words 문서 벡터를 만들어 보는 실습을 진행하겠습니다.

영화 리뷰 데이터인 `text.txt`에 저장되어 있는 `IMDB dataset`을 사용하여 각 리뷰별 문서 벡터를 만들어 보세요.

## 지시사항

1. `text.txt`의 리뷰 데이터를 `documents` 리스트에 추가하세요.
   - 리뷰 데이터를 `documents`에 저장 시, `regex` 변수에 주어진 정규표현식을 사용하여 전처리된 데이터를 저장하세요.
2. `CountVectorizer()` 객체를 생성하세요. 그리고 `fit_transform()` 함수에 1번에서 생성한 `documents`를 사용하여 Bag of words 문서 행렬을 만드세요. 생성된 문서 행렬은 변수 `X`에 저장하세요.
3. 2번에서 생성된 Bag of words 문서 행렬의 **차원**을 변수 `dim`에 저장하세요.
   - `scikit-learn`으로 생성된 문서 행렬의 차원은 `행렬.shape`로 조회할 수 있습니다.
4. 생성한 `CountVectorizer()` 객체에 `get_feature_names()` 메소드를 사용하면, 문서 벡터의 칼럼이 각각 어떤 단어를 나타내는지 확인할 수 있는 리스트가 반환됩니다. 이 중 **첫 10개 칼럼**이 의미하는 단어를 `words_feature` 변수에 저장하세요.
5. 생성한 `CountVectorizer()` 객체에 `vocabulary_[단어]` 메소드를 사용하면, 해당 단어를 의미하는 칼럼의 인덱스를 조회할 수 있습니다. 단어 “comedy”의 빈도수를 나타내는 칼럼 인덱스를 변수 `idx`에 저장하세요.
6. 첫 번째 문서의 Bag of words 벡터를 변수 `vec1`에 저장하세요. 인덱싱을 사용하여 행렬 내 벡터에 접근할 수 있습니다.



## 2. TF-IDF Bag of words 기반 문서 벡터 생성

이번 실습에서는 `scikit-learn`의 `TfidfVectorizer`를 사용하여, TF-IDF 기반 bag of words 문서 벡터를 만들어 보는 실습을 진행하겠습니다. `TfidfVectorizer`의 사용법은 `CountVectorizer`의 사용법과 동일합니다.

영화 리뷰 데이터인 `text.txt`에 저장되어 있는 `IMDB dataset`을 사용하여 각 리뷰별 문서 벡터를 만들어 보세요.

## 지시사항

1. `TfidfVectorizer()` 객체를 생성하고 `fit_transform()` 메소드를 사용하여 TF-IDF 기반 Bag of words 문서 행렬을 생성하여 변수 `X`에 저장하세요.
2. TF-IDF Bag of words 문서 행렬의 차원을 변수 `dim1`에 저장하세요.
3. 첫 번째 문서의 TF-IDF Bag-of-word 벡터를 변수 `vec1`에 저장하세요.
4. `TfidfVectorizer(ngram_range=(1, 2))`으로 객체를 생성하고 `fit_transform()` 메소드를 사용하여 TF-IDF 기반 Bag of N-grams 문서 행렬을 생성하세요.
   - `ngram_range=(1, 2)`는 데이터 내 unigram과 bigram을 사용하여 문서 벡터를 생성한다는 의미를 갖고 있습니다.
5. TF-IDF Bag of N-grams 문서 행렬의 차원을 변수 `dim2`에 저장하세요.



## 3. Bag of words 기반 문서 유사도 측정 서비스

앞서 학습한 TF-IDF 기반 Bag of words 모델을 사용하여 주어진 문서의 유사도를 코사인 유사도를 사용하여 계산하겠습니다.

## 지시사항

1. 학습된 TF-IDF 기반 Bag of words의 `TfidfVectorizer()`객체와 문서 벡터는 `bow_models.pkl`에 저장되어 있습니다. 저장된 모델을 불러와서 객체와 벡터를 각각`vectorizer`와 `X`에 저장하세요.
   - `vectorizer, X = pickle.load(f)`의 형태로 불러올 수 있습니다.
2. 불러온 `vectorizer` 객체에 `transform(문장)` 메소드를 사용하여, 변수`sent1`에 들어있는 문서의 bag of words 벡터를 생성하고 `vec1`에 저장해주세요.
3. 2번 문제와 동일한 방법으로 변수`sent2`에 들어있는 문서의 bag of words 벡터를 생성하여 `vec2`에 저장해주세요.
4. `scikit-learn`의 `cosine_similarity` 함수를 사용하여, `vec1`과 `vec2`의 **코사인 유사도**를 변수 `sim1`에 저장하세요.
5. `scikit-learn`의 `cosine_similarity` 함수를 사용하여, `vec1`과 문서 행렬 `X`의 첫 번째 문서 벡터 간 **코사인 유사도**를 변수 `sim2`에 저장하세요.



## 4. 임베딩을 통한 문장 유사도 측정 서비스

이번 실습에서는 `gensim`을 사용하여, `doc2vec` 문서 벡터를 학습하고, 이를 통해서 문서 간 유사도를 계산해 봅니다. 영화 리뷰 데이터인 `IMDB dataset`을 학습 데이터로 사용합니다.

## 지시사항

1. `text.txt`에 저장되어 있는 `IMDB dataset`을 불러오는 함수 `load_data()`는 미리 구성되어 있습니다. 함수의 내용을 확인하세요.
2. 벡터 간 코사인 유사도를 계산해 주는 함수 `cal_cosine_sim` 함수를 완성하세요.
3. `documents` 리스트에서는 `load_data()` 함수로 불러온 문서 데이터가 저장되어 있습니다. 이를 사용하여 **`window`는 2, `vector_size`는 50, `epochs`는 5**인 doc2vec 모델을 학습하세요.
   - `gensim`으로 doc2vec을 학습하는 방식은 word2vec 학습과 동일합니다. `Word2Vec` 객체 대신 `Doc2Vec` 객체를 만들어서 학습을 진행하세요.
4. 학습된 모델로 `doc1`과 `doc2`에 들어있는 문서의 임베딩 벡터를 생성하여 각각 변수 `vector1`과 `vector2`에 저장하세요.
   - 학습된 `Doc2Vec` 객체에 `infer_vector(문장)` 메소드를 사용하시면 벡터를 생성할 수 있습니다.
5. 위에서 생성된 벡터로 `doc1`과 `doc2`의 **코사인 유사도**를 변수 `sim`에 저장하세요.



## 5. N-gram 언어 모델

이번 실습에서는 변수 `data`에 주어진 문장을 사용해서 간단한 bi-gram 기반 언어 모델을 직접 만들어 보겠습니다.

## 지시사항

1. 입력되는 문서에서 발생하는 모든 **unigram**의 빈도수를 딕셔너리 `unigram_counter`에 저장하여 반환하는 `count_unigram()` 함수를 완성하세요.
2. 입력되는 문서에서 발생하는 모든 **bigram**의 빈도수를 딕셔너리 `bigram_counter`에 저장하여 반환하는 `count_bigram()` 함수를 완성하세요.
3. 입력되는 문장의 발생 확률을 계산하는 `cal_prob()` 함수를 완성하세요.
   - 본 함수는 예측할 문장, unigram의 빈도수 딕셔너리, bigram의 빈도수 딕셔너리를 매개변수로 받습니다. 계산되는 확률값은 `result`에 저장하여 반환하세요.
4. 3번에서 완성된 함수를 사용하여, “this is elice”라는 문장의 발생 확률을 계산하여 확인해봅니다.

### Tips!

- `zip(리스트 1, 리스트 2)` 함수를 사용하면, `[((리스트 1의 원소), (리스트 2의 원소))]`와 같이 두 리스트를 하나의 리스트로 만들 수 있습니다.



## 6. RNN 기반 언어 모델을 통한 간단한 챗봇 서비스 구성

이번 실습에서는 미리 학습된 RNN 기반의 언어 모델을 불러와서 문장 생성을 진행하겠습니다.

본 언어 모델은 셰익스피어의 작품 내 극중 인물들의 대사로 구성되어 있는 `Shakespeare` 데이터셋을 사용하여 학습되었습니다.

## 지시사항

1. 학습된 언어 모델의 파라미터는 `checkpoints` 폴더 아래에 저장되었습니다. `model.load_weights()`를 이용해 주어진 폴더에 저장되어 있는 데이터를 불러오세요.
   - `model.load_weights()` 인자로 `tensorflow`의 `tf.train.latest_checkpoint(경로명)` 함수를 사용하여, 경로 내 존재하는 파라미터 파일에서 데이터를 읽어올 수 있습니다.
2. `generate_text`의 인자로 `model`과 "Juliet: "이라는 문자열을 추가하여 생성된 문장을 `result` 변수에 저장하세요.